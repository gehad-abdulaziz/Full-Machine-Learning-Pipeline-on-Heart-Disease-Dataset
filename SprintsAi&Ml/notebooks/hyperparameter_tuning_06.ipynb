{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11d8c49d",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "Hyperparameter tuning is the process of optimizing the parameters of a machine learning model that are not learned from the data but set prior to training. These parameters greatly affect the model's performance and generalization ability.\n",
    "\n",
    "In this section, we use techniques such as Grid Search Cross-Validation to systematically search for the best combination of hyperparameters for models like Decision Trees and Random Forests. This helps improve prediction accuracy and model robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae97cc",
   "metadata": {},
   "source": [
    "## Load Data and Prepare for Modeling\n",
    "In this step, we:\n",
    "\n",
    "Load the preprocessed and scaled dataset from a CSV file.\n",
    "\n",
    "Separate the features (X) from the target variable (y).\n",
    "\n",
    "Prepare the data for further modeling and hyperparameter tuning.\n",
    "\n",
    "This sets up the data so that machine learning models can be trained and evaluated efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0defa844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "df = pd.read_csv('../Data/selected_features_scaled.csv') \n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29343877",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Logistic Regression\n",
    "In this step, we:\n",
    "\n",
    "Split the dataset into training and testing sets.\n",
    "\n",
    "Scale the features using StandardScaler for better model performance.\n",
    "\n",
    "Define a parameter grid to tune the regularization strength C and the solver method.\n",
    "\n",
    "Use GridSearchCV with 5-fold cross-validation to find the best hyperparameters for Logistic Regression.\n",
    "\n",
    "Print out the best parameters and the best cross-validation accuracy score.\n",
    "\n",
    "This process helps us find the most effective model settings to improve prediction accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b448792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¶ Logistic Regression Best Parameters: {'C': 1, 'solver': 'lbfgs'}\n",
      "ðŸ”¶ Logistic Regression Best CV Score: 0.6198979591836735\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"ðŸ”¶ Logistic Regression Best Parameters:\", grid_lr.best_params_)\n",
    "print(\"ðŸ”¶ Logistic Regression Best CV Score:\", grid_lr.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5453423",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Decision Tree Classifier\n",
    "In this step, we:\n",
    "\n",
    "Define a parameter grid to tune the decision treeâ€™s maximum depth and the minimum number of samples required to split a node.\n",
    "\n",
    "Use GridSearchCV with 5-fold cross-validation to search for the best combination of hyperparameters.\n",
    "\n",
    "Fit the model on the training data and evaluate each parameter set.\n",
    "\n",
    "Print the best parameters found and the corresponding cross-validation accuracy score.\n",
    "\n",
    "This tuning improves the model's generalization and prevents overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "586ebabf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¶ Decision Tree Best Parameters: {'max_depth': 5, 'min_samples_split': 5}\n",
      "ðŸ”¶ Decision Tree Best CV Score: 0.5537414965986395\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_dt = GridSearchCV(DecisionTreeClassifier(), param_grid_dt, cv=5, scoring='accuracy')\n",
    "grid_dt.fit(X_train, y_train)\n",
    "\n",
    "print(\"ðŸ”¶ Decision Tree Best Parameters:\", grid_dt.best_params_)\n",
    "print(\"ðŸ”¶ Decision Tree Best CV Score:\", grid_dt.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae79d72",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Random Forest Classifier\n",
    "In this step, we:\n",
    "\n",
    "Define a grid of hyperparameters to tune:\n",
    "\n",
    "Number of trees (n_estimators)\n",
    "\n",
    "Maximum depth of each tree (max_depth)\n",
    "\n",
    "Minimum number of samples required to split a node (min_samples_split)\n",
    "\n",
    "Use GridSearchCV with 5-fold cross-validation to exhaustively search for the best combination.\n",
    "\n",
    "Fit the random forest model on the training data for each hyperparameter combination.\n",
    "\n",
    "Output the best hyperparameters and their corresponding mean cross-validation accuracy.\n",
    "\n",
    "This process helps improve the model's predictive performance by selecting optimal parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb522bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¶ Random Forest Best Parameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "ðŸ”¶ Random Forest Best CV Score: 0.6321428571428571\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5, scoring='accuracy')\n",
    "grid_rf.fit(X_train, y_train)\n",
    "\n",
    "print(\"ðŸ”¶ Random Forest Best Parameters:\", grid_rf.best_params_)\n",
    "print(\"ðŸ”¶ Random Forest Best CV Score:\", grid_rf.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239d81d",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning for Support Vector Machine (SVM)\n",
    "In this section, we:\n",
    "\n",
    "Define a grid of hyperparameters to tune:\n",
    "\n",
    "Regularization parameter C controls the trade-off between achieving a low training error and a low testing error.\n",
    "\n",
    "Kernel type, choosing between 'linear' and 'rbf' (Radial Basis Function), which affects how the data is transformed.\n",
    "\n",
    "Use GridSearchCV with 5-fold cross-validation to test all combinations of these parameters.\n",
    "\n",
    "Fit the SVM model on the training data with each parameter combination.\n",
    "\n",
    "Print out the best hyperparameters found and the corresponding cross-validation accuracy score.\n",
    "\n",
    "This tuning helps optimize the SVMâ€™s ability to classify the data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "809b6288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¶ SVM Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "ðŸ”¶ SVM Best CV Score: 0.6116496598639456\n"
     ]
    }
   ],
   "source": [
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "grid_svm = GridSearchCV(SVC(), param_grid_svm, cv=5, scoring='accuracy')\n",
    "grid_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"ðŸ”¶ SVM Best Parameters:\", grid_svm.best_params_)\n",
    "print(\"ðŸ”¶ SVM Best CV Score:\", grid_svm.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc84cd3",
   "metadata": {},
   "source": [
    "## Summary of Best Models and Their Performance\n",
    "This code prints a concise summary of the best hyperparameters and cross-validation accuracy scores found for each of the four classification models:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Decision Tree\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Support Vector Machine (SVM)\n",
    "\n",
    "It helps quickly compare which model and parameter settings performed best during tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d7bd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¶ Logistic Regression Best Parameters: {'C': 1, 'solver': 'lbfgs'}\n",
      "ðŸ”¶ Logistic Regression Best CV Score: 0.6198979591836735\n",
      "ðŸ”¶ Decision Tree Best Parameters: {'max_depth': 5, 'min_samples_split': 5}\n",
      "ðŸ”¶ Decision Tree Best CV Score: 0.5578231292517006\n",
      "ðŸ”¶ Random Forest Best Parameters: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "ðŸ”¶ Random Forest Best CV Score: 0.6238945578231292\n",
      "ðŸ”¶ SVM Best Parameters: {'C': 0.1, 'kernel': 'linear'}\n",
      "ðŸ”¶ SVM Best CV Score: 0.6116496598639456\n",
      "\n",
      "âœ… Summary of Best Models:\n",
      "Logistic Regression: {'C': 1, 'solver': 'lbfgs'}  | Accuracy: 0.6198979591836735\n",
      "Decision Tree: {'max_depth': 5, 'min_samples_split': 5}  | Accuracy: 0.5578231292517006\n",
      "Random Forest: {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 100}  | Accuracy: 0.6238945578231292\n",
      "SVM: {'C': 0.1, 'kernel': 'linear'}  | Accuracy: 0.6116496598639456\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nâœ… Summary of Best Models:\")\n",
    "print(\"Logistic Regression:\", grid_lr.best_params_, \" | Accuracy:\", grid_lr.best_score_)\n",
    "print(\"Decision Tree:\", grid_dt.best_params_, \" | Accuracy:\", grid_dt.best_score_)\n",
    "print(\"Random Forest:\", grid_rf.best_params_, \" | Accuracy:\", grid_rf.best_score_)\n",
    "print(\"SVM:\", grid_svm.best_params_, \" | Accuracy:\", grid_svm.best_score_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
